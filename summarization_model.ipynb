{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fe_SA3n0I3T8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPRegressor as mlp\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "NTagdmneHA5f",
    "outputId": "680346c0-d790-48f5-f24c-8e675d26ce4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in /home/priyank/.local/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: six in /home/priyank/.local/lib/python3.6/site-packages (from rouge) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Pb1g4I5ARGTp",
    "outputId": "193e13df-565b-4b70-bbe6-15f5fd776022"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UKpe_KPRLrn"
   },
   "outputs": [],
   "source": [
    "# change to path to dataset\n",
    "file_name = \"/home/priyank/Downloads/Summarization_Pickled_Data-20200506T174927Z-001/Summarization_Pickled_Data/cnn_dataset_1000_labelled.pkl\"\n",
    "stories = pickle.load(open(file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpKVZKCERXbM"
   },
   "outputs": [],
   "source": [
    "# displaying the first datapoint\n",
    "# verify correctness of load\n",
    "\n",
    "# Uncomment to Display the First Datapoint\n",
    "# print(stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "VsEbuI08RZ9J",
    "outputId": "9a67edd2-6283-40e9-a289-a784f14d9a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# Required Models for glove\n",
    "# in case of errors with conda, use this:\n",
    "# conda install -c conda-forge spacy\n",
    "# this is what worked for me :P\n",
    "\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_lg\n",
    "!python -m spacy link en_core_web_lg en --force\n",
    "\n",
    "# use the large model as the default model for English textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-_iuFDvZTvi"
   },
   "outputs": [],
   "source": [
    "# Initializing the processor\n",
    "embedder = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEsDrJHUTOxY"
   },
   "outputs": [],
   "source": [
    "# basic embeddings using averaged glove vectors\n",
    "# using Spacy's large language model\n",
    "def get_embedding(text):\n",
    "    extract = embedder(text)\n",
    "    total_sum = np.zeros(300)\n",
    "    count = 0\n",
    "    for token in extract:\n",
    "        count += 1\n",
    "        total_sum += np.asarray(token.vector)\n",
    "    return total_sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert embeddings using pretrained bert\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import GPT2Tokenizer\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len = 1600)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def get_embedding_bert_sent(text):\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "  # # Tokenize our sentence with the BERT tokenizer.\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "  # # Map the token strings to their vocabulary indeces.\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "  #Mark tokens according to the sentences they belong to\n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "  # Convert inputs to PyTorch tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  # Load pre-trained model (weights)\n",
    "  model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "  # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "  model.eval()\n",
    "\n",
    "  # Predict hidden states features for each layer\n",
    "  with torch.no_grad():\n",
    "      encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "  # Concatenate the tensors for all layers. We use `stack` here to\n",
    "  # create a new dimension in the tensor.\n",
    "  token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "  # Remove dimension 1, the \"batches\".\n",
    "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "  # Swap dimensions 0 and 1.\n",
    "  token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "  # `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "\n",
    "  # `token_vecs` is a tensor with shape [22 x 768]\n",
    "  token_vecs = encoded_layers[11][0]\n",
    "\n",
    "  # Calculate the average of all 22 token vectors.\n",
    "  sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "  return sentence_embedding.numpy()\n",
    "\n",
    "  # print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "  # print(sentence_embedding)\n",
    "\n",
    "def get_embedding_bert_doc(list_doc):\n",
    "  #text = sentence\n",
    "  flag_token_id = 1\n",
    "  segments_ids = []\n",
    "  indexed_tokens = []\n",
    "  for text in list_doc:\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Tokenize our sentence with the BERT tokenizer.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    for i in tokenizer.convert_tokens_to_ids(tokenized_text):\n",
    "      indexed_tokens.append(i)\n",
    "\n",
    "    #Mark tokens according to the sentences they belong to\n",
    "    for i in range(len(tokenized_text)):\n",
    "      segments_ids.append(flag_token_id)\n",
    "\n",
    "    if flag_token_id == 1:\n",
    "      flag_token_id = 0\n",
    "\n",
    "    else:\n",
    "      flag_token_id = 1\n",
    "\n",
    "  # Convert inputs to PyTorch tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  # Load pre-trained model (weights)\n",
    "  model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "  # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "  model.eval()\n",
    "\n",
    "  # Predict hidden states features for each layer\n",
    "  with torch.no_grad():\n",
    "      encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "  # create a new dimension in the tensor.\n",
    "  token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "  # Remove dimension 1, the \"batches\".\n",
    "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "  # Swap dimensions 0 and 1.\n",
    "  token_embeddings = token_embeddings.permute(1,0,2)\n",
    "  token_vecs = encoded_layers[11][0]\n",
    "  # Calculate the average of all 22 token vectors.\n",
    "  document_embedding = torch.mean(token_vecs, dim=0)\n",
    "  return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "26X4T5jHRLpw",
    "outputId": "44af2b50-587a-4f15-f3b8-0ad5fb0960ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# creating the inputs and expected outputs\n",
    "train_size = 100\n",
    "val_size = 20\n",
    "test_size = 20\n",
    "\n",
    "def make_set(start_index, size):\n",
    "    count = 0\n",
    "    X_set = []\n",
    "    y_set = []\n",
    "\n",
    "    for count in tqdm(range(size)):\n",
    "        data = stories[start_index + count]\n",
    "        list_sent_emb = []\n",
    "        for sent in data['story']:\n",
    "              list_sent_emb.append(get_embedding_bert_sent(sent))\n",
    "\n",
    "        doc_emb_list = np.array(list_sent_emb)\n",
    "        doc_emb = np.mean(doc_emb_list, axis=0)\n",
    "#         doc_emb = get_embedding(data['story_text'])\n",
    "        # use the function of choice to generate the document embedding\n",
    "\n",
    "        index = 0\n",
    "        for sentence in data['story']:\n",
    "#             sent_emb = get_embedding(sentence)\n",
    "            sent_emb = get_embedding_bert_sent(sentence)\n",
    "            # use the function of choice to generate the sentence embedding\n",
    "\n",
    "            x = np.concatenate((sent_emb, doc_emb))\n",
    "            try:\n",
    "                y = data['scores'][index]\n",
    "            except:\n",
    "                y = 0.0\n",
    "            index += 1\n",
    "\n",
    "            X_set.append(x)\n",
    "            y_set.append(y)\n",
    "\n",
    "    return np.asmatrix(X_set), np.asarray(y_set)\n",
    "print(\"done\")\n",
    "X_train, y_train = make_set(0, train_size)\n",
    "print(\"done train\")\n",
    "X_val, y_val = make_set(train_size, val_size)\n",
    "X_test, y_test = make_set(train_size + val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PY6JkXtWWOwm"
   },
   "outputs": [],
   "source": [
    "def get_values(X, model):\n",
    "    return model.predict(X)\n",
    "\n",
    "def get_loss(pred, y):\n",
    "    return np.linalg.norm(pred - y) / np.shape(y)[0]\n",
    "\n",
    "model_name = \"glove_averaged\"\n",
    "# modify the model name\n",
    "\n",
    "def make_parameters(train_size):\n",
    "    batch_size = int(4 * np.sqrt(train_size))\n",
    "    n_batches = int(32 * (train_size / batch_size))\n",
    "    # can set batch_size to standard values such as 64, 128, 256\n",
    "\n",
    "    print(\"Total Number of Training Examples: \" + str(train_size))\n",
    "    print(\"Batch Size: \" + str(batch_size))\n",
    "    print(\"Number of Batches: \" + str(n_batches))\n",
    "\n",
    "    return batch_size, n_batches\n",
    "\n",
    "def train(X_train, y_train, batch_size, n_batches):\n",
    "    model = mlp(hidden_layer_sizes = (1024, 2048, 1024, 512, 256, 256, 128, 64), max_iter = 10000)\n",
    "    \n",
    "    train_size = np.shape(X_train)[0]\n",
    "\n",
    "    min_loss = 1e20\n",
    "\n",
    "    for iterator in tqdm(range(n_batches)):\n",
    "        idx = np.random.randint(0, train_size, size = batch_size)\n",
    "\n",
    "        X_select = X_train[idx,:]\n",
    "        y_select = y_train[idx]\n",
    "\n",
    "        model.partial_fit(X_select, y_select)\n",
    "\n",
    "        sentence_predicted_scores = get_values(X_val, model)\n",
    "\n",
    "        loss = get_loss(sentence_predicted_scores, y_val)\n",
    "\n",
    "        # saving best model seen so far\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            pickle.dump(model, open(model_name + '_best_model', 'wb'))\n",
    "\n",
    "    final_model = pickle.load(open(model_name + '_best_model', 'rb'))\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "UrX21q4u8ukA",
    "outputId": "9f49bb17-3e30-46f6-a0dd-beb4de345f1f"
   },
   "outputs": [],
   "source": [
    "batch_size, n_batches = make_parameters(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yZzWH2VWaQjZ",
    "outputId": "b0a68bf1-db74-4582-8c25-b78f837c4e1c"
   },
   "outputs": [],
   "source": [
    "m = train(X_train, 1000 * y_train, batch_size, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jl4yFkiyiTa_"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter for similarity threshold\n",
    "theta = 0.95\n",
    "\n",
    "def similarity(A, B):\n",
    "    similarity =  (A @ B.T) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "    return similarity\n",
    "\n",
    "def get_top_k(X_doc, y, k):\n",
    "    # k should be in {3, 4, 5}\n",
    "    # error handling\n",
    "    k = int(k)\n",
    "    if k > 5:\n",
    "        k = 5\n",
    "    elif k < 3:\n",
    "        k = 3\n",
    "    \n",
    "    order = np.flip(np.argsort(y))\n",
    "    sentence_set = []\n",
    "    for sent_id in order:\n",
    "        if sentence_set == []:\n",
    "            sentence_set.append(order[0])\n",
    "            continue\n",
    "\n",
    "        consider = X_doc[sent_id, :]\n",
    "        flag = 1\n",
    "        for consider_id in sentence_set:\n",
    "            if similarity(X_doc[consider_id, :], consider) > theta:\n",
    "                flag = 0\n",
    "                break\n",
    "\n",
    "        if flag == 1:\n",
    "            sentence_set.append(sent_id)\n",
    "    return sentence_set[0: min(k, len(sentence_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_zjPWDVG179"
   },
   "outputs": [],
   "source": [
    "# Creating object of the ROUGE class\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "ph6OFgGwcewS",
    "outputId": "bcb27a9a-a061-4a79-dbf2-e6d0cb97c0f0"
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# testing out each document iteratively\n",
    "# test set: document 'train_size + val_size' onwards\n",
    "\n",
    "def join(lst):\n",
    "    string = \"\"\n",
    "    for elem in lst:\n",
    "        string = string + elem + \" . \"\n",
    "    return string\n",
    "\n",
    "def extract_rouge(rouge_dict):\n",
    "    scores = []\n",
    "\n",
    "    scores.append(100 * rouge_dict[\"rouge-1\"]['f'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-1\"]['p'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-1\"]['r'])\n",
    "\n",
    "    scores.append(100 * rouge_dict[\"rouge-2\"]['f'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-2\"]['p'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-2\"]['r'])\n",
    "\n",
    "    scores.append(100 * rouge_dict[\"rouge-l\"]['f'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-l\"]['p'])\n",
    "    scores.append(100 * rouge_dict[\"rouge-l\"]['r'])\n",
    "\n",
    "    return np.asarray(scores)\n",
    "\n",
    "start_doc_id = train_size + val_size\n",
    "doc_count = len(stories)\n",
    "\n",
    "generated_summary, gold_summary = 0, 0\n",
    "# to access the final created summary\n",
    "\n",
    "# set the number of documents for testing\n",
    "limit = test_size\n",
    "\n",
    "result = {}\n",
    "result['3'] = np.zeros(9)\n",
    "result['4'] = np.zeros(9)\n",
    "result['5'] = np.zeros(9)\n",
    "# averaging the ROUGE Metrics\n",
    "# for different summary lengths\n",
    "\n",
    "count = 0\n",
    "\n",
    "while count < min(doc_count, limit):\n",
    "    X_doc = []\n",
    "    y_doc = []\n",
    "    data = stories[start_doc_id + count]\n",
    "    doc_emb = get_embedding(data['story_text'])\n",
    "\n",
    "    index = 0\n",
    "    for sentence in data['story']:\n",
    "        sent_emb = get_embedding(sentence)\n",
    "\n",
    "        x = np.concatenate((sent_emb, doc_emb))\n",
    "        try:\n",
    "            y = data['scores'][index]\n",
    "        except:\n",
    "            y = 0.0\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        X_doc.append(x)\n",
    "        y_doc.append(y)\n",
    "\n",
    "    X_doc = np.asmatrix(X_doc)\n",
    "    y_doc = np.asarray(y_doc)\n",
    "\n",
    "    sentence_predicted_scores = get_values(X_doc, m)\n",
    "\n",
    "    loss = np.linalg.norm(sentence_predicted_scores - y_doc)\n",
    "\n",
    "    # Uncomment to view the test_loss on the sample  \n",
    "    # print(loss)\n",
    "\n",
    "    gold_summary = join(data['highlights'])\n",
    "\n",
    "    for k in [3, 4, 5]:\n",
    "        summary_sent_id = get_top_k(X_doc, sentence_predicted_scores, k)\n",
    "        # Uncomment to view the indices of chosen sentences\n",
    "        # print(\"Document ID:\", start_doc_id + count, \", Top 5 Sentences:\", summary_sent_id)\n",
    "\n",
    "        # Uncomment to view the top 10 sentences based on Gold Labels\n",
    "        # print(\"Top 10 sentences based on Gold Label\", np.ndarray.tolist(np.flip(np.argsort(y_doc))[0:10]))\n",
    "\n",
    "        generated_summary = join([data['story'][idx] for idx in summary_sent_id])\n",
    "\n",
    "        scores = rouge.get_scores(generated_summary, gold_summary)[0]\n",
    "        result[str(k)] += extract_rouge(scores)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    result[str(k)] = result[str(k)] / test_size\n",
    "\n",
    "predicted = get_values(X_test, m)\n",
    "test_loss = get_loss(y_test, predicted)\n",
    "\n",
    "print(\"Sample Output:\")\n",
    "print(\"Document:\\n\", stories[-1]['story_text'])\n",
    "print(\"Generated Summary:\\n\", generated_summary)\n",
    "print(\"Gold Summary:\\n\", gold_summary)\n",
    "\n",
    "print(\"\\nAll Metrics:\\n\")\n",
    "\n",
    "data = []\n",
    "for k in [3, 4, 5]:\n",
    "    lst = np.ndarray.tolist(result[str(k)])\n",
    "    lst.append(test_loss)\n",
    "    data.append(lst)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['R1-f', 'R1-p', 'R1-r',\n",
    "                                    'R2-f', 'R2-p', 'R2-r',\n",
    "                                    'Rl-f', 'Rl-p', 'Rl-r',\n",
    "                                    'Loss'], dtype = float)\n",
    "\n",
    "df.index = ['glove top-3', 'glove top-4', 'glove top-5']\n",
    "display(df)\n",
    "\n",
    "# save results into a dataframe file\n",
    "df.to_csv(model_name + '_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uZR5MRhhD9f"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "summarization_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
