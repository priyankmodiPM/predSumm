# -*- coding: utf-8 -*-
"""Copy of summarization_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lOfzJ_Oh6q03UuIFUb962z_GmXp_D7hy
"""

import pickle
import spacy
import nltk
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.neural_network import MLPRegressor as mlp

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# change to path to dataset
file_name = "/content/drive/My Drive/Summarization_Pickled_Data/cnn_dataset_1000_labelled.pkl"
stories = pickle.load(open(file_name, 'rb'))

# displaying the first datapoint
# verify correctness of load
print(stories[0])

# Required Models for glove
# in case of errors with conda, use this:
# conda install -c conda-forge spacy
# this is what worked for me :P

!python -m spacy download en
!python -m spacy download en_core_web_lg
!python -m spacy link en_core_web_lg en --force

# use the large model as the default model for English textual data

# Initializing the processor
embedder = spacy.load('en')

!pip install pytorch-pretrained-bert

# basic embeddings using averaged glove vectors
# using Spacy's large language model
def get_embedding(text):
    extract = embedder(text)
    total_sum = np.zeros(300)
    count = 0
    for token in extract:
        count += 1
        total_sum += np.asarray(token.vector)
    return total_sum / count

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# bert embeddings using pretrained bert

import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
from transformers import GPT2Tokenizer
# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows
import logging
#logging.basicConfig(level=logging.INFO)

import matplotlib.pyplot as plt
# % matplotlib inline

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len = 1600)
# tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

def get_embedding_bert_sent(text):
  marked_text = "[CLS] " + text + " [SEP]"

  # # Tokenize our sentence with the BERT tokenizer.
  tokenized_text = tokenizer.tokenize(marked_text)

  # # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
  # indexed_tokens = tokenizer.encode(text, max_length=2)
  # print('length of indexed tokens', len(indexed_tokens))
  #Mark tokens according to the sentences they belong to
  segments_ids = [1] * len(tokenized_text)
  # segments_ids = [1,1,1,1,1,1,1,1,1]
  # print(segments_ids)
  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens])
  segments_tensors = torch.tensor([segments_ids])

  # Load pre-trained model (weights)
  model = BertModel.from_pretrained('bert-base-uncased')

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  model.eval()

  # Predict hidden states features for each layer
  with torch.no_grad():
      encoded_layers, _ = model(tokens_tensor, segments_tensors)

  # print ("Number of layers:", len(encoded_layers))
  # layer_i = 0

  # print ("Number of batches:", len(encoded_layers[layer_i]))
  # batch_i = 0

  # print ("Number of tokens:", len(encoded_layers[layer_i][batch_i]))
  # token_i = 0

  # print ("Number of hidden units:", len(encoded_layers[layer_i][batch_i][token_i]))

  # Concatenate the tensors for all layers. We use `stack` here to
  # create a new dimension in the tensor.
  token_embeddings = torch.stack(encoded_layers, dim=0)

  # Remove dimension 1, the "batches".
  token_embeddings = torch.squeeze(token_embeddings, dim=1)

  # Swap dimensions 0 and 1.
  token_embeddings = token_embeddings.permute(1,0,2)

  # `encoded_layers` has shape [12 x 1 x 22 x 768]

  # `token_vecs` is a tensor with shape [22 x 768]
  token_vecs = encoded_layers[11][0]

  # Calculate the average of all 22 token vectors.
  sentence_embedding = torch.mean(token_vecs, dim=0)

  return sentence_embedding

  # print ("Our final sentence embedding vector of shape:", sentence_embedding.size())
  # print(sentence_embedding)

def get_embedding_bert_doc(list_doc):
  #text = sentence
  flag_token_id = 1
  segments_ids = []
  indexed_tokens = []
  for text in list_doc:
    marked_text = "[CLS] " + text + " [SEP]"

    # Tokenize our sentence with the BERT tokenizer.
    tokenized_text = tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    for i in tokenizer.convert_tokens_to_ids(tokenized_text):
      indexed_tokens.append(i)

    #Mark tokens according to the sentences they belong to
    for i in range(len(tokenized_text)):
      segments_ids.append(flag_token_id)

    if flag_token_id == 1:
      flag_token_id = 0

    else:
      flag_token_id = 1

  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens])
  segments_tensors = torch.tensor([segments_ids])

  # Load pre-trained model (weights)
  model = BertModel.from_pretrained('bert-base-uncased')

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  model.eval()

  # Predict hidden states features for each layer
  with torch.no_grad():
      encoded_layers, _ = model(tokens_tensor, segments_tensors)

  # create a new dimension in the tensor.
  token_embeddings = torch.stack(encoded_layers, dim=0)
  # Remove dimension 1, the "batches".
  token_embeddings = torch.squeeze(token_embeddings, dim=1)
  # Swap dimensions 0 and 1.
  token_embeddings = token_embeddings.permute(1,0,2)
  token_vecs = encoded_layers[11][0]
  # Calculate the average of all 22 token vectors.
  document_embedding = torch.mean(token_vecs, dim=0)
  return document_embedding

data = stories[0]
print(len(data['story']))
print(data['story'][10])
# get_embedding_bert_doc(['"lets do this people"', 'do you wanna build a snowman', 'yes lets build a snowman', 'why are you not building', data['story'][30]])
# get_embedding_bert_doc(data['story'])
get_embedding_bert_sent('do you wanna build a snowman')
# count = 0
# for i in data['story']:
#   get_embedding_bert_sent(i)
#   print(count)
#   count +=

# data['story'][-1]

# creating the inputs and expected outputs
X_train = []
y_train = []
count = 0
list_sent_emb = []
for data in stories:
    count += 1
    # doc_emb = get_embedding(data['story_text'])
    # doc_emb = get_embedding_bert_doc(data['story'])
    for sent in data['story']:
      list_sent_emb.append(get_embedding_bert_sent(sent))
    doc_emb = torch.mean(torch.stack(list_sent_emb))
    print('doc emb size', doc_emb.size())
    # use the function of choice to generate the document embedding

    index = 0
    for sentence in data['story']:
        # sent_emb = get_embedding(sentence)
        sent_emb = get_embedding_bert_sent(sentence)
        # use the function of choice to generate the sentence embedding

        x = np.concatenate((sent_emb, doc_emb))
        y = data['scores'][index] 
        index += 1

        X_train.append(x)
        y_train.append(y)

    if count > 100:
        break

X_train = np.asmatrix(X_train)
y_train = np.asarray(y_train)

def train(X, y):
    model = mlp(hidden_layer_sizes = (1024, 2048, 1024, 512, 256), max_iter = 100)
    model.fit(X, y)
    return model

def get_values(X, model):
    return model.predict(X)

m = train(X_train, 1000 * y_train)

# Hyperparameter for similarity threshold
theta = 0.95

def similarity(A, B):
    similarity =  (A @ B.T) / (np.linalg.norm(A) * np.linalg.norm(B))
    return similarity

def get_top_5(X_doc, y):
    order = np.flip(np.argsort(y))
    sentence_set = []
    for sent_id in order:
        if sentence_set == []:
            sentence_set.append(order[0])
            continue

        consider = X_doc[sent_id, :]
        flag = 1
        for consider_id in sentence_set:
            if similarity(X_doc[consider_id, :], consider) > theta:
                flag = 0
                break

        if flag == 1:
            sentence_set.append(sent_id)
    return sentence_set[0: min(5, len(sentence_set))]

# evaluation
# testing out each document iteratively
# test set: document 950 onwards

doc_id = 950
doc_count = len(stories)

# set the number of documents for testing
limit = 960

while doc_id < min(doc_count, limit):
    X_doc = []
    y_doc = []
    data = stories[doc_id]
    doc_emb = get_embedding(data['story_text'])

    index = 0
    for sentence in data['story']:
        sent_emb = get_embedding(sentence)

        x = np.concatenate((sent_emb, doc_emb))
        y = data['scores'][index] 

        index += 1

        X_doc.append(x)
        y_doc.append(y)

    X_doc = np.asmatrix(X_doc)
    y_doc = np.asarray(y_doc)

    sentence_predicted_scores = get_values(X_doc, m)

    loss = np.linalg.norm(sentence_predicted_scores - y_doc)

    # Uncomment to view the test_loss on the sample  
    # print(loss)

    print("Document ID:", doc_id, ", Top 5 Sentences:", get_top_5(X_doc, sentence_predicted_scores))

    # Uncomment to view the top 10 sentences based on Gold Labels
    print("Top 10 sentences based on Gold Label", np.flip(np.argsort(y_doc))[0:10])
    doc_id += 1

# ^_^ Thank You