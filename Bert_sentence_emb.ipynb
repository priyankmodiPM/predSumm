{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert-sentence_emb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQrsfoT44tv",
        "colab_type": "code",
        "outputId": "338900ac-e262-48da-82e4-aa49b576d527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.47)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.47->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED6f79Be47aj",
        "colab_type": "code",
        "outputId": "cd0002f0-bd20-4ec6-84f1-1ce92911b250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1160234.02B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM0qQpGY49oY",
        "colab_type": "code",
        "outputId": "fac116f3-7d14-4747-9df2-79aefcb53b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e4ttgfX5DUT",
        "colab_type": "code",
        "outputId": "32d2f8b2-f210-447c-d49e-8dd82c86c835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5LGK5Y5kHd",
        "colab_type": "code",
        "outputId": "fbe06d28-4da1-4738-d82a-cf357b0ef316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H8THxit54CF",
        "colab_type": "code",
        "outputId": "4f3d10da-2970-4396-8601-51b50e0d974c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 36153128.45B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXN0LStK56am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6OsCv1n6Avr",
        "colab_type": "code",
        "outputId": "6b73cec9-6210-4a08-ef50-27323a44a31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print (\"Number of layers:\", len(encoded_layers))\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers: 12\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT4WL1306HPi",
        "colab_type": "code",
        "outputId": "4651c779-f319-437d-abf6-5bf73360453e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGuiTgex6L4K",
        "colab_type": "code",
        "outputId": "8637e383-e8d0-4099-a446-75954c9e1023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# `encoded_layers` is a Python list.\n",
        "print('     Type of encoded_layers: ', type(encoded_layers))\n",
        "\n",
        "# Each layer in the list is a torch tensor.\n",
        "print('Tensor shape for each layer: ', encoded_layers[0].size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Type of encoded_layers:  <class 'list'>\n",
            "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR1ehuLZ6ULK",
        "colab_type": "code",
        "outputId": "25bb82f9-72e5-4a46-cd35-97f6c2917287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "token_embeddings.size()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 1, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXu2ah3e6XZo",
        "colab_type": "code",
        "outputId": "dcc63491-9fdb-4880-fb6d-d05ca07501d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 22, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_y0hsKn6a5q",
        "colab_type": "code",
        "outputId": "047d7910-51e9-4996-da55-7c43f9c150f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 12, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91RKXiUh6lcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = encoded_layers[11][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iBlX5Lo6wmF",
        "colab_type": "code",
        "outputId": "dca29e12-fe7a-4138-9136-683f2c11a823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
        "print(sentence_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n",
            "tensor([ 3.2873e-02, -2.3461e-01, -7.9924e-02,  3.8905e-01,  8.8793e-01,\n",
            "         2.1375e-01, -7.8063e-03,  6.2694e-01, -3.2631e-02, -3.4703e-01,\n",
            "         1.2331e-01, -9.4772e-02, -7.4420e-02,  4.5523e-01, -4.7217e-01,\n",
            "         1.0344e-01,  3.4667e-01,  1.0410e-01,  5.4372e-01,  6.9113e-02,\n",
            "        -8.2880e-02,  6.7866e-02,  1.2871e-01,  2.3312e-01,  4.2933e-01,\n",
            "        -1.2449e-02, -2.1360e-01,  2.2697e-01, -1.2763e-01,  2.8080e-01,\n",
            "         5.4766e-01, -1.0080e-01,  7.5650e-02, -2.7152e-01, -1.4124e-01,\n",
            "        -4.0931e-01, -1.9237e-01, -3.9813e-02, -2.3175e-01,  3.3040e-01,\n",
            "        -3.8484e-01, -3.7469e-01, -2.4978e-01,  3.2707e-01, -9.0552e-04,\n",
            "        -4.4285e-01,  7.6833e-02, -4.4618e-02,  1.9808e-02,  7.4679e-02,\n",
            "        -3.2986e-01,  8.2114e-01, -7.6256e-01, -4.1610e-04,  8.9090e-02,\n",
            "         5.3472e-01, -3.8244e-01, -6.0184e-01, -8.3941e-02, -9.6009e-02,\n",
            "         3.7689e-01, -2.5287e-01,  4.6429e-01, -5.4761e-01, -2.1230e-02,\n",
            "         2.0828e-02,  4.8191e-01,  2.8035e-01, -3.2533e-01,  3.0219e-02,\n",
            "        -4.0341e-01,  1.9626e-01, -2.1912e-01, -3.6637e-01,  6.2737e-03,\n",
            "         4.7330e-01, -8.0409e-02, -2.9862e-02, -2.9353e-01,  2.1191e-01,\n",
            "        -2.0604e-01,  3.9470e-01, -1.4167e-01,  2.4277e-01,  1.4530e-01,\n",
            "         5.1124e-02, -1.8740e-01,  7.7009e-02, -3.3881e-02,  4.4765e-01,\n",
            "        -5.5695e-02,  1.3957e-01, -1.9834e-01, -3.4934e-02, -4.2185e-01,\n",
            "         2.1998e-01,  1.4355e-02,  9.0102e-02, -2.7612e-01,  4.6219e-01,\n",
            "         1.0327e-01, -5.5465e-01,  2.5412e-01,  4.0143e-01, -2.3759e-03,\n",
            "        -1.0715e-02,  3.4206e-01,  6.8675e-02, -2.1206e-01, -9.5972e-02,\n",
            "         1.2728e-01, -2.8885e-01,  2.1959e-01, -2.6426e-01,  2.9848e-02,\n",
            "        -1.0109e-03,  4.3754e-01, -3.0179e-01, -5.3250e-01, -1.1406e-01,\n",
            "         4.0874e-01,  2.4168e-01,  3.4493e-01,  8.7342e-01, -2.4015e-01,\n",
            "         2.1779e-01,  2.8373e-02,  1.6783e-01, -1.3983e-01, -6.2004e-01,\n",
            "         3.9955e-01,  2.8869e-02,  1.7543e-01, -1.8415e-01, -2.5643e-01,\n",
            "         2.2322e-01, -1.1652e-01, -2.4955e-01, -5.3888e-01,  2.5615e-01,\n",
            "         3.0306e-01, -3.5355e-01, -8.1194e-02,  2.9323e-01,  6.4744e-02,\n",
            "         3.9605e-01,  2.6805e-01, -2.7199e-01,  1.9049e-01,  2.2043e-01,\n",
            "         2.6581e-01,  1.7944e-01, -1.3411e-01, -2.6965e-01,  3.5733e-01,\n",
            "         2.2490e-01,  1.3277e-02,  4.0494e-02, -1.3025e-01, -1.3364e-01,\n",
            "         3.2034e-01,  2.6117e-01, -3.4705e-01,  1.7669e-01, -2.5380e-01,\n",
            "         3.0491e-02,  2.4185e-01,  2.4148e-01, -2.8340e-01,  7.3602e-02,\n",
            "        -3.1289e-01,  4.5428e-02,  5.6594e-01, -6.4136e-02,  1.3558e-01,\n",
            "         1.1851e-01, -2.3538e-01, -1.3218e-01,  1.9183e-01, -1.3600e-01,\n",
            "        -1.1494e+00,  3.6584e-01,  3.1123e-01,  2.8061e-01,  2.3955e-01,\n",
            "        -2.4946e-01,  3.0123e-01,  9.9572e-02, -2.3740e-01, -4.4925e-01,\n",
            "         5.2637e-02, -5.1481e-01, -6.5989e-01,  1.1719e-01, -6.3273e-02,\n",
            "        -1.1423e-01, -2.6833e-01, -1.9182e-02, -2.4028e-01, -4.4373e-02,\n",
            "        -1.7569e-01, -3.1761e-02,  7.9137e-02,  7.1695e-02, -6.7586e-02,\n",
            "         8.7838e-03,  2.0592e-01,  2.1971e-02,  3.6809e-01,  2.0450e-01,\n",
            "        -2.8241e-01,  2.8737e-01,  2.3896e-01,  3.5003e-03, -4.0584e-02,\n",
            "        -3.7527e-02, -3.1873e-01, -4.4533e-01, -1.8830e-02,  8.7480e-02,\n",
            "         3.9358e-02,  2.5384e-01, -1.7177e-01,  3.6704e-01,  1.1685e-01,\n",
            "         8.8151e-01, -1.8297e-01, -2.2382e-01, -2.5170e-01,  2.3097e-01,\n",
            "        -1.8420e-01, -1.2809e-01,  2.5144e-01, -1.3566e-01, -2.7711e-01,\n",
            "        -1.7238e-01, -2.2674e-01, -1.0894e-01, -2.2153e-01, -3.2150e-01,\n",
            "         5.1438e-02,  8.1830e-02,  4.4762e-01, -1.8171e-01,  1.7812e-01,\n",
            "         4.2392e-02,  2.0953e-01,  5.9237e-01, -1.5180e-01, -2.4848e-01,\n",
            "        -3.6314e-01, -2.7957e-01, -2.6763e-01, -4.0601e-01,  2.2221e-01,\n",
            "        -2.3289e-01,  2.8370e-02, -8.3567e-03, -6.9577e-02,  1.8851e-01,\n",
            "         5.0710e-01, -3.0316e-02, -9.0177e-02, -1.0818e-03, -2.5721e-01,\n",
            "        -4.3441e-01, -1.9286e-01,  2.4037e-01,  1.5816e-01,  2.6835e-01,\n",
            "         2.3048e-01,  1.8329e-01, -1.4023e-02,  4.2113e-01, -2.5815e-02,\n",
            "        -2.8211e-01, -4.1548e-02,  4.7248e-03,  4.9955e-02, -4.2047e-02,\n",
            "         2.4905e-01,  5.3173e-01, -4.2934e-01, -3.2588e-01,  1.0013e-01,\n",
            "        -3.6364e-01, -9.9431e-02,  6.2659e-01, -2.7112e-01, -2.1566e-01,\n",
            "        -1.5469e-01,  8.3550e-02, -3.6112e-01,  9.6514e-02, -1.7314e-01,\n",
            "         2.1588e-01,  1.5639e-01,  5.9367e-01,  6.6649e-01, -6.7588e-02,\n",
            "         1.5030e-01,  1.4506e-01,  2.5358e-01, -3.9685e-02, -2.3935e-01,\n",
            "         9.3357e-02,  4.7813e-01, -3.0064e-01, -3.7770e+00,  7.7552e-01,\n",
            "         4.0279e-01, -3.5019e-01,  3.8938e-02, -1.2376e-01, -1.1008e-01,\n",
            "        -4.8648e-01, -2.5933e-01,  1.7168e-01, -4.2279e-01, -5.5471e-01,\n",
            "         3.1611e-01, -1.1100e-01,  1.2557e-01, -4.5863e-01,  1.9956e-01,\n",
            "        -2.3371e-01,  1.4228e-01,  1.3801e-01, -1.9041e-01, -2.0123e-01,\n",
            "        -2.1891e-01,  1.7641e-01,  5.5051e-01,  4.2103e-01, -4.9933e-01,\n",
            "        -1.3517e-01,  2.4512e-01,  1.2218e-01,  1.2216e-01, -2.2440e-01,\n",
            "        -2.3644e-01, -4.1047e-01,  1.9044e-01,  7.0886e-02, -1.6605e-01,\n",
            "        -1.2517e-01, -1.0968e-02,  1.9421e-01, -2.5221e-01, -2.2930e-01,\n",
            "        -1.6755e-01,  4.9772e-02,  9.7385e-01,  1.3127e-01, -1.7879e-01,\n",
            "        -5.1492e-01,  4.2442e-02,  2.5898e-01,  4.4694e-01, -3.0948e-01,\n",
            "        -2.2084e-01,  2.9276e-01,  9.6879e-02, -8.7703e-03,  2.0275e-01,\n",
            "         3.3256e-02, -1.4174e-01, -4.4820e-02, -6.2446e-02, -4.6275e-01,\n",
            "        -1.0691e-01,  8.4085e-02, -9.6781e-03, -3.4852e-01, -3.5904e-01,\n",
            "        -2.2667e-03,  2.3287e-01,  8.2754e-03,  8.3930e-02,  3.9229e-01,\n",
            "        -3.9168e-01, -9.0533e-01, -3.2820e-01, -9.6777e-04,  6.3286e-01,\n",
            "        -2.0340e-01,  2.1092e-01, -5.2722e-02, -2.5226e-01, -1.9713e-01,\n",
            "         2.2695e-01, -1.2394e-01,  2.8051e-02, -3.8859e-01, -1.2037e-01,\n",
            "         3.0186e-01, -3.8589e-01, -6.7337e-01,  4.1308e-01,  3.9467e-01,\n",
            "         1.7403e-01, -5.0731e-02, -3.9394e-02,  6.4084e-02, -2.5928e-01,\n",
            "        -2.0895e-01, -6.6158e-03,  6.7344e-02, -5.3995e-02, -6.8231e-02,\n",
            "         3.9435e-01, -1.6228e-01, -5.9148e-02, -1.0987e-01, -2.3485e-01,\n",
            "         6.0764e-02, -5.2708e-01,  3.1985e-01,  1.5007e-01, -5.3241e-01,\n",
            "         7.4603e-01,  2.9623e-03,  7.0720e-02,  2.9006e-01,  5.8697e-01,\n",
            "         4.5578e-01,  7.8198e-02,  3.2700e-03, -2.5953e-01,  4.7365e-01,\n",
            "        -4.7452e-01, -3.1938e-01,  6.3678e-02, -1.2565e-01,  1.0425e-01,\n",
            "        -1.1837e-01,  8.1148e-02, -3.1782e-01,  8.7375e-02, -3.9548e-01,\n",
            "         1.2083e-01,  8.9850e-02,  3.7519e-01, -1.2269e-02,  2.2328e-01,\n",
            "        -1.1208e+00,  6.4218e-02,  3.6587e-01, -1.7193e-01,  2.0568e-01,\n",
            "        -1.2611e-02, -3.2089e-01, -2.5760e-01,  1.0754e-01,  3.6568e-02,\n",
            "        -6.1796e-02, -7.8660e-02, -8.2236e-02, -4.3350e-01, -9.8674e-04,\n",
            "         1.0287e-01, -5.2039e-01, -8.4610e-02,  3.9558e-01,  2.5404e-01,\n",
            "        -9.4516e-02, -5.6194e-01, -2.3692e-02,  9.7243e-02,  2.2519e-01,\n",
            "         3.9128e-01,  3.5818e-02, -9.8754e-01,  2.2243e-01, -4.9979e-01,\n",
            "        -2.7877e-01, -2.2256e-01,  7.1030e-01,  5.6814e-02, -2.4126e-01,\n",
            "         4.5316e-02,  9.8688e-02,  7.2637e-03,  3.4653e-01,  2.5113e-02,\n",
            "        -2.6737e-01, -2.0366e-01,  4.3638e-02, -3.6745e-01,  2.3696e-01,\n",
            "        -2.5555e-01, -2.9447e-01,  3.2046e-01, -4.0940e-02, -2.4763e-02,\n",
            "         8.6968e-02,  5.1339e-02, -1.8419e-04, -1.4893e-02,  3.2526e-01,\n",
            "        -3.8269e-02,  2.6786e-01, -3.5656e-01, -4.5695e-01,  1.3729e-01,\n",
            "         1.4438e-01,  2.3524e-02,  3.8044e-01,  1.5785e-01,  1.7076e-02,\n",
            "        -1.7419e-01, -9.5848e-02, -1.5767e-01, -3.8592e-01,  3.0318e-01,\n",
            "         1.5875e-01, -3.8609e-01,  4.4404e-01, -2.9244e-01,  2.0231e-02,\n",
            "        -6.4094e-01, -5.7448e-01,  1.5995e-01,  2.7726e-02,  4.6958e-01,\n",
            "         1.2975e-01, -2.8781e-01, -1.3774e-01, -3.7001e-01,  1.5315e-02,\n",
            "         1.5106e-01,  4.9228e-02, -6.1478e-01, -2.2820e-01,  1.9403e-01,\n",
            "         2.3295e-01,  3.1816e-02,  3.6159e-02, -2.0600e-02, -1.5040e-02,\n",
            "        -1.8070e-01,  2.5212e-01,  3.3567e-01,  1.7605e-01, -1.7763e-01,\n",
            "        -6.6003e-01, -1.5966e-01, -7.5785e-02, -9.2748e-02,  4.1193e-01,\n",
            "        -1.4701e-01,  1.8071e-01,  2.7727e-01,  2.2886e-01,  2.1412e-01,\n",
            "         4.0559e-01,  2.9488e-02,  1.5939e-01,  8.9484e-02,  1.7819e-02,\n",
            "         1.2578e-01,  2.1416e-01, -6.3731e-01, -1.3492e-01, -3.4915e-01,\n",
            "         3.6478e-02, -1.4420e-01,  2.2444e-01,  8.0618e-02, -4.5948e-01,\n",
            "        -2.5548e-01,  2.3308e-01,  4.8316e-01, -4.8303e-02,  1.3733e-01,\n",
            "         3.8448e-01,  1.9859e-01, -1.1588e-01,  1.9462e-01, -5.7137e-01,\n",
            "        -2.1805e-02,  1.8119e-02, -1.8648e-01,  3.2569e-01, -3.1400e-02,\n",
            "        -7.4852e-01, -2.8136e-01, -4.1346e-01, -4.3298e-01,  8.8853e-02,\n",
            "         8.0123e-02, -9.6356e-02, -1.0290e-01, -6.7089e-02, -1.8135e-02,\n",
            "         3.8983e-01, -2.2420e-01, -6.5782e-01,  1.6714e-01,  1.6647e-01,\n",
            "         2.2787e-01, -9.8913e-02, -4.9981e-02,  2.4047e-01,  5.1464e-01,\n",
            "         3.0415e-01,  1.9503e-01,  3.9066e-01, -7.7427e-02,  2.1399e-01,\n",
            "        -4.9991e-02,  3.7401e-01, -1.3171e-01,  2.5106e-01, -1.9302e-01,\n",
            "        -2.3373e-01, -1.2552e-01,  9.8667e-02, -2.5677e-01, -6.0451e-01,\n",
            "         2.7188e-01,  5.8789e-02, -4.7728e-01, -2.4809e-01,  1.0637e-01,\n",
            "        -2.4408e-01, -6.8313e-01,  1.5543e-01, -3.8480e-02, -1.0999e-01,\n",
            "         2.4612e-01,  3.1702e-01,  1.5008e-01,  4.7603e-01, -2.1330e-01,\n",
            "         2.5903e-02, -4.4135e-02,  1.8807e-01, -3.2210e-01,  5.7168e-02,\n",
            "        -2.6996e-01,  2.1716e-01, -2.0677e-01, -1.4930e-01,  6.6474e-02,\n",
            "        -3.5995e-01,  2.7575e-01, -3.3963e-01,  1.3209e-01, -1.0043e-01,\n",
            "        -2.8423e-02,  8.8633e-01,  2.6062e-01, -1.0798e-02, -1.2912e-01,\n",
            "        -2.6626e-02,  4.1478e-01,  3.0718e-01,  6.0709e-02,  9.1707e-02,\n",
            "        -4.0817e-02, -5.0389e-01,  6.9781e-01,  2.5277e-01,  2.0832e-01,\n",
            "        -7.7927e-02, -2.7423e-01,  3.0759e-01,  1.4393e-01, -1.9446e-01,\n",
            "         4.0682e-01, -5.2621e-01, -1.8164e-01,  1.4928e-01, -4.0263e-01,\n",
            "        -3.9829e-01, -8.1881e-02, -2.2226e-02,  3.0405e-01,  2.8618e-01,\n",
            "        -2.4212e-01, -4.5130e-01, -9.4955e-02,  4.1823e-01, -1.7606e-01,\n",
            "         2.2215e-01,  2.7881e-02, -1.7553e-01,  3.0276e-04, -2.4626e-01,\n",
            "        -4.1756e-01, -5.0924e-01, -3.4444e-01, -1.3423e-01, -2.6289e-02,\n",
            "        -1.0585e-01, -1.2218e-01, -5.4948e-01,  4.2556e-01, -3.3032e-01,\n",
            "         1.8397e-01, -2.3458e-02,  1.3004e-01, -4.7927e-01,  1.8001e-01,\n",
            "         5.1205e-01,  4.0620e-01, -4.9702e-02,  1.5907e-01, -3.3849e-01,\n",
            "        -1.8257e-02,  5.7774e-03, -3.4494e-01, -5.1504e-03,  1.3520e-01,\n",
            "        -2.2486e-02, -5.2574e-01,  4.5000e-01,  9.1079e-02, -7.8127e-02,\n",
            "        -8.1374e-01,  3.7877e-01, -1.3157e-02, -5.4255e-02,  2.0971e-01,\n",
            "         4.8571e-01, -1.9917e-01,  2.1274e-01, -1.1891e-02, -6.4185e-02,\n",
            "        -1.0941e-01,  1.4496e-01, -4.9473e-01,  4.1608e-01,  4.2309e-02,\n",
            "         5.6278e-02,  5.7971e-02,  7.4634e-02, -1.0295e-01,  3.8489e-02,\n",
            "         4.5559e-02,  2.2024e-01, -1.4470e-01, -3.0052e-01, -2.7554e-01,\n",
            "         5.3516e-01, -2.6329e-01,  1.9391e-01,  4.3121e-01,  1.2750e-02,\n",
            "         2.4978e-01,  8.1130e-02,  1.1838e-01, -4.8755e-02, -4.5545e-01,\n",
            "        -1.7653e-01,  2.3618e-01, -2.3790e-01, -1.9036e-01,  1.8762e-02,\n",
            "        -1.2124e-01, -4.1950e-01, -2.4885e-01, -1.0401e-01,  3.0578e-01,\n",
            "        -1.8370e-01, -8.9854e-02,  3.8368e-02])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1m4KU7-6yNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}